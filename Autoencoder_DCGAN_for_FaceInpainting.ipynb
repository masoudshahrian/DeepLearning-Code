{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 37705,
          "sourceType": "datasetVersion",
          "datasetId": 29561
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Autoencoder-DCGAN-for-FaceInpainting",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masoudshahrian/DeepLearning-Code/blob/main/Autoencoder_DCGAN_for_FaceInpainting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "jessicali9530_celeba_dataset_path = kagglehub.dataset_download('jessicali9530/celeba-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "cu7BnlgvEtPT"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# # For example, here's several helpful packages to load\n",
        "\n",
        "# import numpy as np # linear algebra\n",
        "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# # Input data files are available in the read-only \"../input/\" directory\n",
        "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "# import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "g6LLnBw-EtPU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dlib\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-14T14:37:01.475891Z",
          "iopub.execute_input": "2024-12-14T14:37:01.476593Z",
          "iopub.status.idle": "2024-12-14T14:43:47.412457Z",
          "shell.execute_reply.started": "2024-12-14T14:37:01.476563Z",
          "shell.execute_reply": "2024-12-14T14:43:47.411384Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "_rsluc5zEtPU",
        "outputId": "539e04b3-c161-41b5-c62c-3666f58d21f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting dlib\n  Downloading dlib-19.24.6.tar.gz (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: dlib\n  Building wheel for dlib (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for dlib: filename=dlib-19.24.6-cp310-cp310-linux_x86_64.whl size=3590660 sha256=fbcc7855cb7595356ba4f5235b3b363232630c878da79e19e8495d37150cecc6\n  Stored in directory: /root/.cache/pip/wheels/7c/1d/d1/e69ceb001441acedfa6156acda6c1856699e260ea1a9e6dcc4\nSuccessfully built dlib\nInstalling collected packages: dlib\nSuccessfully installed dlib-19.24.6\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "yRdHSEzFEtPV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# import dlib\n",
        "# import os\n",
        "\n",
        "# # مسیر به دایرکتوری تصاویر CelebA\n",
        "# input_dir = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'  # مسیر دایرکتوری تصاویر\n",
        "# output_dir = '/kaggle/working/CelebA_Image_Croped/'  # مسیر دایرکتوری خروجی\n",
        "\n",
        "# # ایجاد دایرکتوری خروجی در صورت عدم وجود\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# # بارگذاری مدل شناسایی صورت\n",
        "# detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "# # لیست کردن تصاویر در دایرکتوری ورودی\n",
        "# image_count = 0  # شمارنده برای تعداد تصاویر پردازش شده\n",
        "# max_images = 10000  # حداکثر تعداد تصاویری که باید پردازش شوند\n",
        "\n",
        "# for filename in os.listdir(input_dir):\n",
        "#     if (filename.endswith('.jpg') or filename.endswith('.png')) and image_count < max_images:\n",
        "#         # بارگذاری تصویر\n",
        "#         image_path = os.path.join(input_dir, filename)\n",
        "#         image = cv2.imread(image_path)\n",
        "\n",
        "#         # تبدیل تصویر به خاکستری\n",
        "#         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "#         # شناسایی صورت‌ها\n",
        "#         faces = detector(gray)\n",
        "\n",
        "#         # پردازش هر صورت شناسایی شده\n",
        "#         for face in faces:\n",
        "#             # استخراج مختصات صورت\n",
        "#             x, y, w, h = (face.left(), face.top(), face.width(), face.height())\n",
        "\n",
        "#             # محاسبه مرکز صورت\n",
        "#             center_x = x + w // 2\n",
        "#             center_y = y + h // 2\n",
        "\n",
        "#             # محاسبه مختصات جدید برای کراپ\n",
        "#             crop_x1 = max(center_x - 64, 0)\n",
        "#             crop_x2 = min(center_x + 64, image.shape[1])\n",
        "#             crop_y1 = max(center_y - 64, 0)\n",
        "#             crop_y2 = min(center_y + 64, image.shape[0])\n",
        "\n",
        "#             # کراپ کردن تصویر\n",
        "#             cropped_image = image[crop_y1:crop_y2, crop_x1:crop_x2]\n",
        "\n",
        "#             # تغییر اندازه به 128x128\n",
        "#             resized_image = cv2.resize(cropped_image, (128, 128))\n",
        "\n",
        "#             # ذخیره تصویر کراپ شده\n",
        "#             output_path = os.path.join(output_dir, f'cropped_{image_count}.jpg')  # نام فایل خروجی\n",
        "#             cv2.imwrite(output_path, resized_image)\n",
        "\n",
        "#             image_count += 1  # افزایش شمارنده\n",
        "\n",
        "#             # اگر به حداکثر تعداد تصاویر رسیدیم، از حلقه خارج شویم\n",
        "#             if image_count >= max_images:\n",
        "#                 break\n",
        "\n",
        "# print(f\"تمامی {image_count} تصویر با موفقیت کراپ و ذخیره شدند.\")"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "b0KZwMkREtPW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import shutil\n",
        "# import random"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "ldi-S3gLEtPX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # مسیر به دایرکتوری تصاویر ورودی\n",
        "# input_dir = '/kaggle/working/CelebA_Image_Croped/'  # مسیر دایرکتوری تصاویر\n",
        "# train_dir = '/kaggle/working/train/'  # مسیر دایرکتوری train\n",
        "# test_dir = '/kaggle/working/test/'    # مسیر دایرکتوری test"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "zImSXbJPEtPY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # import os\n",
        "# # import shutil\n",
        "# # import random\n",
        "\n",
        "# # # مسیر به دایرکتوری تصاویر ورودی\n",
        "# # input_dir = '/content/drive/MyDrive/kaggle-celebA/images/'  # مسیر دایرکتوری تصاویر\n",
        "# # train_dir = '/content/drive/MyDrive/kaggle-celebA/train/'  # مسیر دایرکتوری train\n",
        "# # test_dir = '/content/drive/MyDrive/kaggle-celebA/test/'    # مسیر دایرکتوری test\n",
        "\n",
        "# # ایجاد دایرکتوری‌های train و test در صورت عدم وجود\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# # لیست کردن تصاویر در دایرکتوری ورودی\n",
        "# images = [f for f in os.listdir(input_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "# # # اطمینان از اینکه تعداد تصاویر 10000 تا است\n",
        "# # if len(images) < 100000:\n",
        "# #     print(f\"تعداد تصاویر موجود کمتر از 20 است: {len(images)}\")\n",
        "# # else:\n",
        "# #     # انتخاب 100000 تصویر تصادفی\n",
        "# #     images = random.sample(images, 100000)\n",
        "\n",
        "# # # تصادفی کردن لیست تصاویر\n",
        "# # random.shuffle(images)\n",
        "\n",
        "# # محاسبه تعداد تصاویر برای train و test\n",
        "# train_size = int(0.8 * len(images))  # 80% برای train\n",
        "# test_size = len(images) - train_size   # 20% برای test\n",
        "\n",
        "# # تقسیم تصاویر به train و test\n",
        "# train_images = images[:train_size]\n",
        "# test_images = images[train_size:]\n",
        "\n",
        "# # کپی کردن تصاویر به دایرکتوری‌های مربوطه\n",
        "# for img in train_images:\n",
        "#     shutil.copy(os.path.join(input_dir, img), os.path.join(train_dir, img))\n",
        "\n",
        "# for img in test_images:\n",
        "#     shutil.copy(os.path.join(input_dir, img), os.path.join(test_dir, img))\n",
        "\n",
        "# print(f\"تعداد تصاویر در train: {len(train_images)}\")\n",
        "# print(f\"تعداد تصاویر در test: {len(test_images)}\")\n",
        "# print(\"تقسیم تصاویر با موفقیت انجام شد.\")"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "gz_EHgvqEtPY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import shutil\n",
        "# import random\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import layers, models\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import cv2\n",
        "# import glob\n",
        "# from skimage.metrics import structural_similarity as ssim"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "JIvXYyzWEtPZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Set your image dataset paths\n",
        "# train_path = '/kaggle/working/train'\n",
        "# test_path = '/kaggle/working/test'"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "UFtkLbi3EtPb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and preprocess the dataset\n",
        "def load_images(path, img_size=(64, 64)):\n",
        "    images = []\n",
        "    for file in glob.glob(path + \"/*.jpg\"):\n",
        "        img = cv2.imread(file)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, img_size)\n",
        "        images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "# Split images into upper and lower halves\n",
        "def split_images(images):\n",
        "    upper_half = []\n",
        "    lower_half = []\n",
        "    for img in images:\n",
        "        h, w, _ = img.shape\n",
        "        upper_half.append(img[:h//2, :])\n",
        "        lower_half.append(img[h//2:, :])\n",
        "    return np.array(upper_half), np.array(lower_half)\n",
        "\n",
        "# Normalize Data\n",
        "def preprocess_images(images):\n",
        "    images = images / 255.0\n",
        "    return images"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "_TrqC5olEtPc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initiate autoencoder\n",
        "# def build_autoencoder(input_shape):\n",
        "#     encoder_input = layers.Input(shape=input_shape)\n",
        "\n",
        "#     # Encoder\n",
        "#     x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#     x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "#     x = layers.Dropout(0.2)(x)  # Dropout layer to reduce overfitting\n",
        "\n",
        "#     x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#     x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "#     x = layers.Dropout(0.2)(x)  # Dropout layer to reduce overfitting\n",
        "\n",
        "#     x = layers.Flatten()(x)\n",
        "#     encoder_output = layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # L2 Regularization\n",
        "#     encoder = models.Model(encoder_input, encoder_output)\n",
        "\n",
        "#     # Decoder\n",
        "#     decoder_input = layers.Input(shape=(256,))\n",
        "#     x = layers.Dense((input_shape[0]//4) * (input_shape[1]//4) * 64, activation='relu')(decoder_input)\n",
        "#     x = layers.Reshape((input_shape[0]//4, input_shape[1]//4, 64))(x)\n",
        "#     x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
        "#     x = layers.UpSampling2D((2, 2))(x)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "\n",
        "#     x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
        "#     x = layers.UpSampling2D((2, 2))(x)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "\n",
        "#     decoder_output = layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "#     decoder = models.Model(decoder_input, decoder_output)\n",
        "\n",
        "#     # Complete Autoencoder\n",
        "#     autoencoder_input = layers.Input(shape=input_shape)\n",
        "#     encoded_img = encoder(autoencoder_input)\n",
        "#     decoded_img = decoder(encoded_img)\n",
        "\n",
        "#     autoencoder = models.Model(autoencoder_input, decoded_img)\n",
        "#     autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "#     return autoencoder"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "FPx5w3A4EtPd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import os\n",
        "\n",
        "# مسیر به دایرکتوری تصاویر CelebA\n",
        "input_dir = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'  # مسیر دایرکتوری تصاویر\n",
        "output_dir = '/kaggle/working/CelebA_Image_Croped/'  # مسیر دایرکتوری خروجی\n",
        "\n",
        "# ایجاد دایرکتوری خروجی در صورت عدم وجود\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# بارگذاری مدل شناسایی صورت\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "# لیست کردن تصاویر در دایرکتوری ورودی\n",
        "image_count = 0  # شمارنده برای تعداد تصاویر پردازش شده\n",
        "max_images = 1000  # حداکثر تعداد تصاویری که باید پردازش شوند\n",
        "\n",
        "for filename in os.listdir(input_dir):\n",
        "    if (filename.endswith('.jpg') or filename.endswith('.png')) and image_count < max_images:\n",
        "        # بارگذاری تصویر\n",
        "        image_path = os.path.join(input_dir, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # تبدیل تصویر به خاکستری\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # شناسایی صورت‌ها\n",
        "        faces = detector(gray)\n",
        "\n",
        "        # پردازش هر صورت شناسایی شده\n",
        "        for face in faces:\n",
        "            # استخراج مختصات صورت\n",
        "            x, y, w, h = (face.left(), face.top(), face.width(), face.height())\n",
        "\n",
        "            # محاسبه مرکز صورت\n",
        "            center_x = x + w // 2\n",
        "            center_y = y + h // 2\n",
        "\n",
        "            # محاسبه مختصات جدید برای کراپ\n",
        "            crop_x1 = max(center_x - 64, 0)\n",
        "            crop_x2 = min(center_x + 64, image.shape[1])\n",
        "            crop_y1 = max(center_y - 64, 0)\n",
        "            crop_y2 = min(center_y + 64, image.shape[0])\n",
        "\n",
        "            # کراپ کردن تصویر\n",
        "            cropped_image = image[crop_y1:crop_y2, crop_x1:crop_x2]\n",
        "\n",
        "            # تغییر اندازه به 128x128\n",
        "            resized_image = cv2.resize(cropped_image, (128, 128))\n",
        "\n",
        "            # ذخیره تصویر کراپ شده\n",
        "            output_path = os.path.join(output_dir, f'cropped_{image_count}.jpg')  # نام فایل خروجی\n",
        "            cv2.imwrite(output_path, resized_image)\n",
        "\n",
        "            image_count += 1  # افزایش شمارنده\n",
        "\n",
        "            # اگر به حداکثر تعداد تصاویر رسیدیم، از حلقه خارج شویم\n",
        "            if image_count >= max_images:\n",
        "                break\n",
        "\n",
        "print(f\"تمامی {image_count} تصویر با موفقیت کراپ و ذخیره شدند.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-14T14:43:47.417134Z",
          "iopub.execute_input": "2024-12-14T14:43:47.417508Z",
          "iopub.status.idle": "2024-12-14T14:44:01.061791Z",
          "shell.execute_reply.started": "2024-12-14T14:43:47.417467Z",
          "shell.execute_reply": "2024-12-14T14:44:01.060858Z"
        },
        "id": "5ffJquU0EtPe",
        "outputId": "81a9b35e-2539-487f-8d63-caca5ad4729e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "تمامی 1000 تصویر با موفقیت کراپ و ذخیره شدند.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# مسیر به دایرکتوری تصاویر ورودی\n",
        "input_dir = '/kaggle/working/CelebA_Image_Croped/'  # مسیر دایرکتوری تصاویر\n",
        "train_dir = '/kaggle/working/train/'  # مسیر دایرکتوری train\n",
        "test_dir = '/kaggle/working/test/'    # مسیر دایرکتوری test\n",
        "\n",
        "# ایجاد دایرکتوری‌های train و test در صورت عدم وجود\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# لیست کردن تصاویر در دایرکتوری ورودی\n",
        "images = [f for f in os.listdir(input_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "# # اطمینان از اینکه تعداد تصاویر 10000 تا است\n",
        "# if len(images) < 100000:\n",
        "#     print(f\"تعداد تصاویر موجود کمتر از 20 است: {len(images)}\")\n",
        "# else:\n",
        "#     # انتخاب 100000 تصویر تصادفی\n",
        "#     images = random.sample(images, 100000)\n",
        "\n",
        "# # تصادفی کردن لیست تصاویر\n",
        "# random.shuffle(images)\n",
        "\n",
        "# محاسبه تعداد تصاویر برای train و test\n",
        "train_size = int(0.8 * len(images))  # 80% برای train\n",
        "test_size = len(images) - train_size   # 20% برای test\n",
        "\n",
        "# تقسیم تصاویر به train و test\n",
        "train_images = images[:train_size]\n",
        "test_images = images[train_size:]\n",
        "\n",
        "# کپی کردن تصاویر به دایرکتوری‌های مربوطه\n",
        "for img in train_images:\n",
        "    shutil.copy(os.path.join(input_dir, img), os.path.join(train_dir, img))\n",
        "\n",
        "for img in test_images:\n",
        "    shutil.copy(os.path.join(input_dir, img), os.path.join(test_dir, img))\n",
        "\n",
        "print(f\"تعداد تصاویر در train: {len(train_images)}\")\n",
        "print(f\"تعداد تصاویر در test: {len(test_images)}\")\n",
        "print(\"تقسیم تصاویر با موفقیت انجام شد.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-14T14:44:01.063975Z",
          "iopub.execute_input": "2024-12-14T14:44:01.064348Z",
          "iopub.status.idle": "2024-12-14T14:44:01.171018Z",
          "shell.execute_reply.started": "2024-12-14T14:44:01.064291Z",
          "shell.execute_reply": "2024-12-14T14:44:01.170185Z"
        },
        "id": "9gl59LNoEtPe",
        "outputId": "3184e497-1d7d-40ba-ca8a-d7724261e64a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "تعداد تصاویر در train: 800\nتعداد تصاویر در test: 200\nتقسیم تصاویر با موفقیت انجام شد.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import glob\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "# Set your image dataset paths\n",
        "train_path = '/kaggle/working/train'\n",
        "test_path = '/kaggle/working/test'\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "def load_images(path, img_size=(64, 64)):\n",
        "    images = []\n",
        "    for file in glob.glob(path + \"/*.jpg\"):\n",
        "        img = cv2.imread(file)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, img_size)\n",
        "        images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "# Split images into upper and lower halves\n",
        "def split_images(images):\n",
        "    upper_half = []\n",
        "    lower_half = []\n",
        "    for img in images:\n",
        "        h, w, _ = img.shape\n",
        "        upper_half.append(img[:h//2, :])\n",
        "        lower_half.append(img[h//2:, :])\n",
        "    return np.array(upper_half), np.array(lower_half)\n",
        "\n",
        "# Normalize Data\n",
        "def preprocess_images(images):\n",
        "    images = images / 255.0\n",
        "    return images\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-14T14:44:01.171916Z",
          "iopub.execute_input": "2024-12-14T14:44:01.172161Z",
          "iopub.status.idle": "2024-12-14T14:44:12.528597Z",
          "shell.execute_reply.started": "2024-12-14T14:44:01.172136Z",
          "shell.execute_reply": "2024-12-14T14:44:12.527895Z"
        },
        "id": "_6FeDf9GEtPf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the Autoencoder\n",
        "def build_autoencoder(input_shape):\n",
        "    encoder_input = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    encoder_output = layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    encoder = models.Model(encoder_input, encoder_output)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_input = layers.Input(shape=(256,))\n",
        "    x = layers.Dense((input_shape[0]//4) * (input_shape[1]//4) * 64, activation='relu')(decoder_input)\n",
        "    x = layers.Reshape((input_shape[0]//4, input_shape[1]//4, 64))(x)\n",
        "    x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    decoder_output = layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    decoder = models.Model(decoder_input, decoder_output)\n",
        "\n",
        "    # Complete Autoencoder\n",
        "    autoencoder_input = layers.Input(shape=input_shape)\n",
        "    encoded_img = encoder(autoencoder_input)\n",
        "    decoded_img = decoder(encoded_img)\n",
        "\n",
        "    autoencoder = models.Model(autoencoder_input, decoded_img)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "# Define the DCGAN\n",
        "def build_gen(zdim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(256 * 32 * 32, input_dim=zdim))  # Adjusted for output size\n",
        "    model.add(layers.Reshape((32, 32, 256)))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(3, kernel_size=3, strides=1, padding='same'))\n",
        "    model.add(layers.Activation('tanh'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_dis(img_shape):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "\n",
        "    model.add(layers.Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "\n",
        "    model.add(layers.Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "def build_gan(gen, dis):\n",
        "    model = models.Sequential()\n",
        "    model.add(gen)\n",
        "    model.add(dis)\n",
        "    return model\n",
        "\n",
        "# Hyperparameters\n",
        "img_rows, img_cols, channels = 128, 128, 3  # Adjust according to your dataset\n",
        "zdim = 100\n",
        "\n",
        "# Load and preprocess the dataset (replace with your own dataset)\n",
        "# For demonstration, we'll use random images\n",
        "train_images = np.random.rand(1000, img_rows, img_cols, channels)  # Replace with your dataset\n",
        "train_images = (train_images - 0.5) * 2  # Normalize to [-1, 1]\n",
        "\n",
        "# Build and train the Autoencoder\n",
        "input_shape = train_images.shape[1:]\n",
        "autoencoder = build_autoencoder(input_shape)\n",
        "autoencoder.fit(train_images, train_images, epochs=20, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Build DCGAN\n",
        "dis_v = build_dis((img_rows, img_cols, channels))\n",
        "dis_v.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "gen_v = build_gen(zdim)\n",
        "dis_v.trainable = False\n",
        "gan_v = build_gan(gen_v, dis_v)\n",
        "gan_v.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Training the GAN\n",
        "def train(iterations, batch_size, interval):\n",
        "    real = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Get random images from the dataset\n",
        "        ids = np.random.randint(0, train_images.shape[0], batch_size)\n",
        "        imgs = train_images[ids]\n",
        "\n",
        "        # Generate random noise for the generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, zdim))  # Generate random noise\n",
        "        gen_imgs = gen_v.predict(noise)  # Generate images using the generator\n",
        "\n",
        "        # Train the Discriminator\n",
        "        dloss_real = dis_v.train_on_batch(imgs, real)\n",
        "        dloss_fake = dis_v.train_on_batch(gen_imgs, fake)\n",
        "\n",
        "        # Combine the losses and accuracies\n",
        "        dloss = 0.5 * (dloss_real[0] + dloss_fake[0])  # Average loss\n",
        "        accuracy = 0.5 * (dloss_real[1] + dloss_fake[1])  # Average accuracy\n",
        "\n",
        "        # Train the Generator\n",
        "        z = np.random.normal(0, 1, (batch_size, zdim))\n",
        "        gloss = gan_v.train_on_batch(z, real)\n",
        "\n",
        "        # Check the type of gloss to ensure it's a scalar\n",
        "        if isinstance(gloss, list):\n",
        "            gloss_value = gloss[0]  # Assuming gloss is a list, take the first element\n",
        "        else:\n",
        "            gloss_value = gloss  # If it's already a scalar\n",
        "\n",
        "        if (iteration + 1) % interval == 0:\n",
        "            print(f\"{iteration + 1} [D loss: {dloss:.4f}, acc: {100.0 * accuracy:.2f}] [G loss: {gloss_value:.4f}]\")\n",
        "\n",
        "\n",
        "        show_images(gen_imgs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "def show_images(gen, zdim=100, n_images=16):\n",
        "    # Generate random noise\n",
        "    z = np.random.normal(0, 1, (n_images, zdim))\n",
        "    gen_imgs = gen.predict(z)\n",
        "\n",
        "    # Scale images to [0, 1]\n",
        "    gen_imgs = (gen_imgs + 1) / 2  # Assuming the generator outputs in the range [-1, 1]\n",
        "\n",
        "    # Create a grid of subplots\n",
        "    fig, axs = plt.subplots(4, 4, figsize=(8, 8), sharey=True, sharex=True)\n",
        "\n",
        "    cnt = 0\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            # Check if the images are grayscale or color\n",
        "            if gen_imgs.shape[-1] == 1:  # Grayscale\n",
        "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
        "            else:  # Color (RGB)\n",
        "                axs[i, j].imshow(gen_imgs[cnt])\n",
        "            axs[i, j].axis('off')  # Hide axes\n",
        "            cnt += 1\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "    plt.show()  # Show the figure\n",
        "# Train the GAN\n",
        "train(200, 32, 10)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-14T14:49:41.893078Z",
          "iopub.execute_input": "2024-12-14T14:49:41.893475Z",
          "iopub.status.idle": "2024-12-14T14:50:13.678551Z",
          "shell.execute_reply.started": "2024-12-14T14:49:41.893443Z",
          "shell.execute_reply": "2024-12-14T14:50:13.677309Z"
        },
        "id": "4xOr38OmEtPf",
        "outputId": "8a062b65-f72d-44eb-8fed-8f283c1611fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 190ms/step - loss: 2.2378 - val_loss: 0.5013\nEpoch 2/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.4885 - val_loss: 0.3525\nEpoch 3/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3980 - val_loss: 0.3374\nEpoch 4/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.3676 - val_loss: 0.3392\nEpoch 5/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3541 - val_loss: 0.3434\nEpoch 6/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.3477 - val_loss: 0.3411\nEpoch 7/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3447 - val_loss: 0.3411\nEpoch 8/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3417 - val_loss: 0.3367\nEpoch 9/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3403 - val_loss: 0.3392\nEpoch 10/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3395 - val_loss: 0.3383\nEpoch 11/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.3375 - val_loss: 0.3370\nEpoch 12/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3371 - val_loss: 0.3377\nEpoch 13/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.3369 - val_loss: 0.3384\nEpoch 14/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3381 - val_loss: 0.3401\nEpoch 15/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3383 - val_loss: 0.3378\nEpoch 16/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3362 - val_loss: 0.3375\nEpoch 17/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3362 - val_loss: 0.3384\nEpoch 18/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3362 - val_loss: 0.3373\nEpoch 19/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3359 - val_loss: 0.3365\nEpoch 20/20\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3360 - val_loss: 0.3363\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 185\u001b[0m\n\u001b[1;32m    183\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()  \u001b[38;5;66;03m# Show the figure\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Train the GAN\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[7], line 151\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(iterations, batch_size, interval)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (iteration \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [D loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] [G loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgloss_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m \u001b[43mshow_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_imgs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[7], line 163\u001b[0m, in \u001b[0;36mshow_images\u001b[0;34m(gen, zdim, n_images)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_images\u001b[39m(gen, zdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, n_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# Generate random noise\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (n_images, zdim))\n\u001b[0;32m--> 163\u001b[0m     gen_imgs \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(z)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Scale images to [0, 1]\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     gen_imgs \u001b[38;5;241m=\u001b[39m (gen_imgs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Assuming the generator outputs in the range [-1, 1]\u001b[39;00m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'predict'"
          ],
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'predict'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Initiate autoencoder\n",
        "# def build_autoencoder(input_shape):\n",
        "#     encoder_input = layers.Input(shape=input_shape)\n",
        "\n",
        "#     # Encoder\n",
        "#     x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#     x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "#     x = layers.Dropout(0.2)(x)  # Dropout layer to reduce overfitting\n",
        "\n",
        "#     x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#     x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "#     x = layers.Dropout(0.2)(x)  # Dropout layer to reduce overfitting\n",
        "\n",
        "#     x = layers.Flatten()(x)\n",
        "#     encoder_output = layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # L2 Regularization\n",
        "#     encoder = models.Model(encoder_input, encoder_output)\n",
        "\n",
        "#     # Decoder\n",
        "#     decoder_input = layers.Input(shape=(256,))\n",
        "#     x = layers.Dense((input_shape[0]//4) * (input_shape[1]//4) * 64, activation='relu')(decoder_input)\n",
        "#     x = layers.Reshape((input_shape[0]//4, input_shape[1]//4, 64))(x)\n",
        "#     x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
        "#     x = layers.UpSampling2D((2, 2))(x)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "\n",
        "#     x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
        "#     x = layers.UpSampling2D((2, 2))(x)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "\n",
        "#     decoder_output = layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "#     decoder = models.Model(decoder_input, decoder_output)\n",
        "\n",
        "#     # Complete Autoencoder\n",
        "#     autoencoder_input = layers.Input(shape=input_shape)\n",
        "#     encoded_img = encoder(autoencoder_input)\n",
        "#     decoded_img = decoder(encoded_img)\n",
        "\n",
        "#     autoencoder = models.Model(autoencoder_input, decoded_img)\n",
        "#     autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "#     return autoencoder\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the Autoencoder\n",
        "def build_autoencoder(input_shape):\n",
        "    encoder_input = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    encoder_output = layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    encoder = models.Model(encoder_input, encoder_output)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_input = layers.Input(shape=(256,))\n",
        "    x = layers.Dense((input_shape[0]//4) * (input_shape[1]//4) * 64, activation='relu')(decoder_input)\n",
        "    x = layers.Reshape((input_shape[0]//4, input_shape[1]//4, 64))(x)\n",
        "    x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    decoder_output = layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    decoder = models.Model(decoder_input, decoder_output)\n",
        "\n",
        "    # Complete Autoencoder\n",
        "    autoencoder_input = layers.Input(shape=input_shape)\n",
        "    encoded_img = encoder(autoencoder_input)\n",
        "    decoded_img = decoder(encoded_img)\n",
        "\n",
        "    autoencoder = models.Model(autoencoder_input, decoded_img)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "# Define the DCGAN\n",
        "def build_gen(zdim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(256*7*7, input_dim=zdim))\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, kernel_size=3, strides=1, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(3, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.Activation('tanh'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_dis(img_shape):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "\n",
        "    model.add(layers.Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "\n",
        "    model.add(layers.Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "def build_gan(gen, dis):\n",
        "    model = models.Sequential()\n",
        "    model.add(gen)\n",
        "    model.add(dis)\n",
        "    return model\n",
        "\n",
        "# Hyperparameters\n",
        "img_rows, img_cols, channels =128 ,128 , 3  # Adjust according to your dataset\n",
        "zdim = 100\n",
        "\n",
        "# Load and preprocess the dataset (replace with your own dataset)\n",
        "# For demonstration, we'll use random images\n",
        "train_images = np.random.rand(1000, img_rows, img_cols, channels)  # Replace with your dataset\n",
        "train_images = (train_images - 0.5) * 2  # Normalize to [-1, 1]\n",
        "\n",
        "# Build and train the Autoencoder\n",
        "input_shape = train_images.shape[1:]\n",
        "autoencoder = build_autoencoder(input_shape)\n",
        "autoencoder.fit(train_images, train_images, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Build DCGAN\n",
        "dis_v = build_dis((img_rows, img_cols, channels))\n",
        "dis_v.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "gen_v = build_gen(zdim)\n",
        "dis_v.trainable = False\n",
        "gan_v = build_gan(gen_v, dis_v)\n",
        "gan_v.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Training the GAN\n",
        "def train(iterations, batch_size, interval):\n",
        "    real = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Get random images from the dataset\n",
        "        ids = np.random.randint(0, train_images.shape[0], batch_size)\n",
        "        imgs = train_images[ids]\n",
        "\n",
        "        # Generate images with the autoencoder\n",
        "        encoded_imgs = autoencoder.predict(imgs)\n",
        "        noise = np.random.normal(0, 0.1, encoded_imgs.shape)  # Adding noise\n",
        "        gen_imgs = gen_v.predict(noise)\n",
        "\n",
        "        # Train the Discriminator\n",
        "        dloss_real = dis_v.train_on_batch(imgs, real)\n",
        "        dloss_fake = dis_v.train_on_batch(gen_imgs, fake)\n",
        "\n",
        "        dloss, accuracy = 0.5 * np.add(dloss_real, dloss_fake)\n",
        "\n",
        "        # Train the Generator\n",
        "        z = np.random.normal(0, 1, (batch_size, zdim))\n",
        "        gloss = gan_v.train_on_batch(z, real)\n",
        "\n",
        "        if (iteration + 1) % interval == 0:\n",
        "            print(f\"{iteration + 1} [D loss: {dloss[0]:.4f}, acc: {100.0 * accuracy:.2f}] [G loss: {gloss:.4f}]\")\n",
        "\n",
        "# Train the GAN\n",
        "train(10, 128, 1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-24T19:21:59.978426Z",
          "iopub.execute_input": "2024-11-24T19:21:59.978754Z",
          "iopub.status.idle": "2024-11-24T19:22:27.846029Z",
          "shell.execute_reply.started": "2024-11-24T19:21:59.978725Z",
          "shell.execute_reply": "2024-11-24T19:22:27.844856Z"
        },
        "collapsed": true,
        "jupyter": {
          "source_hidden": true,
          "outputs_hidden": true
        },
        "id": "GB7KNPoyEtPg",
        "outputId": "5952159b-5295-4213-a64c-4b09918d4387"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1732476125.765869    1476 service.cc:145] XLA service 0x7aa65c0051b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1732476125.765946    1476 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[1m 7/29\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.0813",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "I0000 00:00:1732476132.984709    1476 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 258ms/step - loss: 2.2257 - val_loss: 0.5099\nEpoch 2/10\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.4561 - val_loss: 0.3436\nEpoch 3/10\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3802 - val_loss: 0.3596\nEpoch 4/10\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3665 - val_loss: 0.3402\nEpoch 5/10\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3565 - val_loss: 0.3399\nEpoch 6/10\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3511 - val_loss: 0.3409\nEpoch 7/10\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3495 - val_loss: 0.3463\nEpoch 8/10\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3481 - val_loss: 0.3411\nEpoch 9/10\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3428 - val_loss: 0.3380\nEpoch 10/10\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3418 - val_loss: 0.3432\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step  \n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 189\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [D loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdloss[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] [G loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Train the GAN\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[6], line 173\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(iterations, batch_size, interval)\u001b[0m\n\u001b[1;32m    171\u001b[0m encoded_imgs \u001b[38;5;241m=\u001b[39m autoencoder\u001b[38;5;241m.\u001b[39mpredict(imgs)\n\u001b[1;32m    172\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, encoded_imgs\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Adding noise\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m gen_imgs \u001b[38;5;241m=\u001b[39m \u001b[43mgen_v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Train the Discriminator\u001b[39;00m\n\u001b[1;32m    176\u001b[0m dloss_real \u001b[38;5;241m=\u001b[39m dis_v\u001b[38;5;241m.\u001b[39mtrain_on_batch(imgs, real)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/models/functional.py:288\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    286\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    287\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32, 128, 128, 3), dtype=float32). Expected shape (None, 100), but input has incompatible shape (32, 128, 128, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 128, 128, 3), dtype=float32)\n  • training=False\n  • mask=None"
          ],
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32, 128, 128, 3), dtype=float32). Expected shape (None, 100), but input has incompatible shape (32, 128, 128, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 128, 128, 3), dtype=float32)\n  • training=False\n  • mask=None",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "AtIkzG2pEtPg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the training dataset\n",
        "train_images = load_images(train_path)\n",
        "upper_half_train, lower_half_train = split_images(train_images)\n",
        "upper_half_train = preprocess_images(upper_half_train)\n",
        "lower_half_train = preprocess_images(lower_half_train)\n",
        "\n",
        "# Build and train the autoencoder\n",
        "input_shape = upper_half_train.shape[1:]\n",
        "autoencoder = build_autoencoder(input_shape)\n",
        "\n",
        "# Train the model\n",
        "autoencoder.fit(upper_half_train, lower_half_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Load and preprocess the test dataset\n",
        "test_images = load_images(test_path)\n",
        "upper_half_test, lower_half_test = split_images(test_images)\n",
        "\n",
        "# Normalize test data\n",
        "upper_half_test = upper_half_test / 255.0\n",
        "\n",
        "# Predict the lower half using the model\n",
        "lower_half_pred = autoencoder.predict(upper_half_test)\n",
        "\n",
        "def display_results(model, upper_half_test, lower_half_pred):\n",
        "    predictions = model.predict(upper_half_test)\n",
        "\n",
        "    fig, axes = plt.subplots(3, 10, figsize=(15, 5))\n",
        "\n",
        "    for i in range(10):\n",
        "        # چاپ ابعاد نیمه بالایی و پیش‌بینی\n",
        "        print(f\"Upper half shape: {upper_half_test[i].shape}, Prediction shape: {predictions[i].shape}\")\n",
        "\n",
        "        # نمایش نیمه بالایی\n",
        "        axes[0, i].imshow(upper_half_test[i])\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # نمایش نیمه پایینی\n",
        "        axes[1, i].imshow(lower_half_pred[i])\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "        # ترکیب نیمه بالایی و پیش‌بینی\n",
        "        combined_image = np.vstack((upper_half_test[i], predictions[i]))\n",
        "\n",
        "        # نمایش تصویر ترکیبی\n",
        "        axes[2, i].imshow(combined_image)\n",
        "        axes[2, i].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# فرض کنید که autoencoder، upper_half و lower_half از قبل تعریف شده‌اند\n",
        "display_results(autoencoder, upper_half_test, lower_half_test)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-14T14:50:13.679382Z",
          "iopub.status.idle": "2024-12-14T14:50:13.679688Z",
          "shell.execute_reply.started": "2024-12-14T14:50:13.679548Z",
          "shell.execute_reply": "2024-12-14T14:50:13.679563Z"
        },
        "id": "RWjXldVZEtPg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "pybNOcV_EtPh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load and preprocess the training dataset\n",
        "# train_images = load_images(train_path)\n",
        "# upper_half_train, lower_half_train = split_images(train_images)\n",
        "# upper_half_train = preprocess_images(upper_half_train)\n",
        "# lower_half_train = preprocess_images(lower_half_train)\n",
        "\n",
        "# # Build and train the autoencoder\n",
        "# input_shape = upper_half_train.shape[1:]\n",
        "# autoencoder = build_autoencoder(input_shape)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "KkKypcb9EtPh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "# autoencoder.fit(upper_half_train, lower_half_train, epochs=100, batch_size=32, validation_split=0.1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "zEX4kGuNEtPh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the model\n",
        "# autoencoder.save('autoencoder_model.h5')"
      ],
      "metadata": {
        "trusted": true,
        "id": "Zj8THZWBEtPi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load and preprocess the test dataset\n",
        "# test_images = load_images(test_path)\n",
        "# upper_half_test, lower_half_test = split_images(test_images)\n",
        "\n",
        "# # Normalize test data\n",
        "# upper_half_test = upper_half_test / 255.0\n",
        "\n",
        "# # Predict the lower half using the model\n",
        "# lower_half_pred = autoencoder.predict(upper_half_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "X_T0cPdJEtPi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# def display_results(model, upper_half_test, lower_half_pred):\n",
        "#     predictions = model.predict(upper_half_test)\n",
        "\n",
        "#     fig, axes = plt.subplots(3, 10, figsize=(15, 5))\n",
        "\n",
        "#     for i in range(10):\n",
        "#         # چاپ ابعاد نیمه بالایی و پیش‌بینی\n",
        "#         print(f\"Upper half shape: {upper_half_test[i].shape}, Prediction shape: {predictions[i].shape}\")\n",
        "\n",
        "#         # نمایش نیمه بالایی\n",
        "#         axes[0, i].imshow(upper_half_test[i])\n",
        "#         axes[0, i].axis('off')\n",
        "\n",
        "#         # نمایش نیمه پایینی\n",
        "#         axes[1, i].imshow(lower_half_pred[i])\n",
        "#         axes[1, i].axis('off')\n",
        "\n",
        "#         # ترکیب نیمه بالایی و پیش‌بینی\n",
        "#         combined_image = np.vstack((upper_half_test[i], predictions[i]))\n",
        "\n",
        "#         # نمایش تصویر ترکیبی\n",
        "#         axes[2, i].imshow(combined_image)\n",
        "#         axes[2, i].axis('off')\n",
        "\n",
        "#     plt.show()\n",
        "\n",
        "# # فرض کنید که autoencoder، upper_half و lower_half از قبل تعریف شده‌اند\n",
        "# display_results(autoencoder, upper_half_test, lower_half_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "EDD4WkD0EtPi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the test dataset\n",
        "test_images = load_images(test_path)\n",
        "upper_half_test, lower_half_test = split_images(test_images)\n",
        "upper_half_test = preprocess_images(upper_half_test)\n",
        "lower_half_test = preprocess_images(lower_half_test)\n",
        "\n",
        "# Predict the lower half using the model\n",
        "lower_half_pred = autoencoder.predict(upper_half_test)\n",
        "\n",
        "# DCGAN model\n",
        "def build_dcgan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    dcgan_input = layers.Input(shape=(100,))\n",
        "    x = generator(dcgan_input)\n",
        "    dcgan_output = discriminator(x)\n",
        "    dcgan = models.Model(dcgan_input, dcgan_output)\n",
        "    dcgan.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return dcgan\n",
        "\n",
        "def build_generator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(16*16*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Reshape((16, 16, 256)))\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    return model\n",
        "\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[64, 64, 3]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return model\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "dcgan = build_dcgan(generator, discriminator)\n",
        "\n",
        "# Train DCGAN\n",
        "def train_dcgan(generator, discriminator, dcgan, epochs, batch_size, upper_half_train):\n",
        "    batch_count = upper_half_train.shape[0] // batch_size\n",
        "    for epoch in range(epochs):\n",
        "        for _ in range(batch_count):\n",
        "            noise = np.random.normal(0, 1, [batch_size, 100])\n",
        "            generated_images = generator.predict(noise)\n",
        "\n",
        "            real_images = upper_half_train[np.random.randint(0, upper_half_train.shape[0], size=batch_size)]\n",
        "            real_images = tf.image.resize(real_images, (64, 64))  # Ensure real images have the correct shape\n",
        "\n",
        "            real_images_labels = np.ones((batch_size, 1))\n",
        "            fake_images_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "            discriminator.trainable = True\n",
        "            d_loss_real = discriminator.train_on_batch(real_images, real_images_labels)\n",
        "            d_loss_fake = discriminator.train_on_batch(generated_images, fake_images_labels)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            noise = np.random.normal(0, 1, [batch_size, 100])\n",
        "            valid_y = np.array([1] * batch_size)\n",
        "            discriminator.trainable = False\n",
        "            g_loss = dcgan.train_on_batch(noise, valid_y)\n",
        "\n",
        "        print(f\"{epoch + 1}/{epochs} [D loss: {d_loss}] [G loss: {g_loss}]\")\n",
        "\n",
        "# Start training DCGAN\n",
        "train_dcgan(generator, discriminator, dcgan, epochs=5, batch_size=32, upper_half_train=upper_half_train)"
      ],
      "metadata": {
        "trusted": true,
        "id": "RGfsGDg_EtPj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(autoencoder, generator, upper_half_test):\n",
        "    lower_half_pred = autoencoder.predict(upper_half_test)\n",
        "\n",
        "    # ایجاد ورودی تصادفی برای ژنراتور\n",
        "    noise = np.random.normal(0, 1, (lower_half_pred.shape[0], 100))  # تعداد نمونه‌ها برابر با تعداد پیش‌بینی‌ها\n",
        "\n",
        "    # پیش‌بینی با استفاده از ژنراتور\n",
        "    predictions = generator.predict(noise)  # استفاده از ورودی تصادفی به جای lower_half_pred\n",
        "\n",
        "    fig, axes = plt.subplots(3, 10, figsize=(15, 5))\n",
        "\n",
        "    for i in range(10):\n",
        "        # نمایش نیمه بالایی\n",
        "        axes[0, i].imshow(upper_half_test[i])\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # نمایش نیمه پایینی پیش‌بینی شده (اتوانکودر)\n",
        "        axes[1, i].imshow(lower_half_pred[i])\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "        # نمایش تصویر ترکیبی (نیمه بالایی و نیمه پایینی تولید شده توسط ژنراتور)\n",
        "        generated_image = np.vstack((upper_half_test[i], predictions[i]))  # ترکیب نیمه بالایی و نیمه پایینی\n",
        "        axes[2, i].imshow(generated_image)\n",
        "        axes[2, i].axis('off')\n",
        "\n",
        "    plt.show()\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "JjdauAlMEtPj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the results\n",
        "display_results(autoencoder, generator, upper_half_test)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "z_R5f12_EtPj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# from keras.datasets import mnist\n",
        "# from keras.layers import Activation,BatchNormalization,Dense,Flatten,Reshape\n",
        "# from keras.layers.advanced_activations import LeakyReLU\n",
        "# from keras.layers.convolutional import Conv2D,Conv2DTranspose\n",
        "# from keras.models import Sequential\n",
        "# from keras.optimizers import Adam\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9hg1luVWEtPk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "LeOMQ09LEtPk"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}