{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvuy00BRdhVo2HI2/iEvWM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masoudshahrian/DeepLearning-Code/blob/main/rsna_2024_lumbar_spine_degenerative_classification_with_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------------------\n",
        "# 1. بارگذاری فایل‌های CSV و تعریف مسیرها\n",
        "# ------------------------------\n",
        "train_path = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/'\n",
        "\n",
        "train_df      = pd.read_csv(os.path.join(train_path, 'train.csv'))\n",
        "label_df      = pd.read_csv(os.path.join(train_path, 'train_label_coordinates.csv'))\n",
        "train_desc_df = pd.read_csv(os.path.join(train_path, 'train_series_descriptions.csv'))\n",
        "test_desc_df  = pd.read_csv(os.path.join(train_path, 'test_series_descriptions.csv'))\n",
        "sub           = pd.read_csv(os.path.join(train_path, 'sample_submission.csv'))\n",
        "\n",
        "# ------------------------------\n",
        "# 2. تابع برای تولید مسیرهای تصاویر\n",
        "# ------------------------------\n",
        "def generate_image_paths(df, data_dir):\n",
        "    image_paths = []\n",
        "    for study_id, series_id in zip(df['study_id'], df['series_id']):\n",
        "        study_dir = os.path.join(data_dir, str(study_id))\n",
        "        series_dir = os.path.join(study_dir, str(series_id))\n",
        "        if os.path.exists(series_dir):\n",
        "            images = os.listdir(series_dir)\n",
        "            image_paths.extend([os.path.join(series_dir, img) for img in images])\n",
        "    return image_paths\n",
        "\n",
        "train_image_paths = generate_image_paths(train_desc_df, os.path.join(train_path, 'train_images'))\n",
        "test_image_paths  = generate_image_paths(test_desc_df, os.path.join(train_path, 'test_images'))\n",
        "\n",
        "print(\"نمونه مسیر از تصاویر train:\", train_image_paths[2])\n",
        "print(\"تعداد ردیف‌های train_desc:\", len(train_desc_df))\n",
        "print(\"تعداد تصاویر train:\", len(train_image_paths))\n",
        "\n",
        "# ------------------------------\n",
        "# 3. تغییر شکل داده‌های train و ادغام دیتا فریم‌ها\n",
        "# ------------------------------\n",
        "def reshape_row(row):\n",
        "    data = {'study_id': [], 'condition': [], 'level': [], 'severity': []}\n",
        "    for column, value in row.items():\n",
        "        if column not in ['study_id', 'series_id', 'instance_number', 'x', 'y', 'series_description']:\n",
        "            parts = column.split('_')\n",
        "            condition = ' '.join([word.capitalize() for word in parts[:-2]])\n",
        "            level = parts[-2].capitalize() + '/' + parts[-1].capitalize()\n",
        "            data['study_id'].append(row['study_id'])\n",
        "            data['condition'].append(condition)\n",
        "            data['level'].append(level)\n",
        "            data['severity'].append(value)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "new_train_df = pd.concat([reshape_row(row) for _, row in train_df.iterrows()], ignore_index=True)\n",
        "\n",
        "merged_df = pd.merge(new_train_df, label_df, on=['study_id', 'condition', 'level'], how='inner')\n",
        "final_merged_df = pd.merge(merged_df, train_desc_df, on=['series_id','study_id'], how='inner')\n",
        "\n",
        "final_merged_df['row_id'] = (final_merged_df['study_id'].astype(str) + '_' +\n",
        "                               final_merged_df['condition'].str.lower().str.replace(' ', '_') + '_' +\n",
        "                               final_merged_df['level'].str.lower().str.replace('/', '_'))\n",
        "\n",
        "final_merged_df['image_path'] = (os.path.join(train_path, 'train_images') + '/' +\n",
        "                                 final_merged_df['study_id'].astype(str) + '/' +\n",
        "                                 final_merged_df['series_id'].astype(str) + '/' +\n",
        "                                 final_merged_df['instance_number'].astype(str) + '.dcm')\n",
        "\n",
        "# تغییر برچسب severity به حروف کوچک\n",
        "final_merged_df['severity'] = final_merged_df['severity'].map({\n",
        "    'Normal/Mild': 'normal_mild',\n",
        "    'Moderate': 'moderate',\n",
        "    'Severe': 'severe'\n",
        "})\n",
        "\n",
        "# فیلتر کردن ردیف‌هایی که مسیر تصویر موجود است\n",
        "def check_exists(path):\n",
        "    return os.path.exists(path)\n",
        "final_merged_df = final_merged_df[final_merged_df['image_path'].apply(check_exists)]\n",
        "\n",
        "# نگاشت برچسب‌ها به اعداد صحیح\n",
        "severity_map = {'normal_mild': 0, 'moderate': 1, 'severe': 2}\n",
        "final_merged_df['severity'] = final_merged_df['severity'].map(severity_map)\n",
        "\n",
        "# استفاده از final_merged_df به عنوان داده‌های آموزشی\n",
        "train_data = final_merged_df.copy()\n",
        "\n",
        "# ------------------------------\n",
        "# 4. توابع بارگذاری و پیش‌پردازش تصاویر DICOM\n",
        "# ------------------------------\n",
        "def load_dicom_image(path):\n",
        "    \"\"\"\n",
        "    تابعی برای بارگذاری تصویر DICOM\n",
        "    \"\"\"\n",
        "    # تبدیل EagerTensor به آرایه NumPy و سپس به رشته\n",
        "    path = path.numpy().decode('utf-8')\n",
        "    ds = pydicom.dcmread(path)\n",
        "    data = ds.pixel_array.astype(np.float32)\n",
        "    data = data - np.min(data)\n",
        "    if np.max(data) != 0:\n",
        "        data = data / np.max(data)\n",
        "    data = (data * 255).astype(np.uint8)\n",
        "    return data\n",
        "\n",
        "def load_and_preprocess(path, label=None):\n",
        "    \"\"\"\n",
        "    - بارگذاری تصویر DICOM با استفاده از tf.py_function\n",
        "    - افزودن بعد کانال (برای تصاویر خاکستری)\n",
        "    - تغییر اندازه به 224x224، تبدیل از grayscale به RGB و نرمال‌سازی به [0, 1]\n",
        "    \"\"\"\n",
        "    image = tf.py_function(func=lambda p: load_dicom_image(p), inp=[path], Tout=tf.uint8)\n",
        "    image.set_shape([None, None])\n",
        "    # افزودن بعد کانال (از (ارتفاع, عرض) به (ارتفاع, عرض, 1))\n",
        "    image = tf.expand_dims(image, axis=-1)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image = tf.image.grayscale_to_rgb(image)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    if label is None:\n",
        "        return image\n",
        "    else:\n",
        "        return image, label\n",
        "\n",
        "# ------------------------------\n",
        "# 5. ایجاد دیتاست‌های TensorFlow برای هر سری توضیحی\n",
        "# ------------------------------\n",
        "def create_datasets(df, series_description, batch_size=8):\n",
        "    filtered_df = df[df['series_description'] == series_description]\n",
        "    if filtered_df.empty:\n",
        "        raise ValueError(f\"داده‌ای برای سری توضیحی: {series_description} پیدا نشد.\")\n",
        "    train_df_part, val_df_part = train_test_split(filtered_df, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_paths = train_df_part['image_path'].values\n",
        "    train_labels = train_df_part['severity'].values\n",
        "    val_paths = val_df_part['image_path'].values\n",
        "    val_labels = val_df_part['severity'].values\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "    train_ds = train_ds.map(lambda p, l: load_and_preprocess(p, l),\n",
        "                            num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    train_ds = train_ds.shuffle(buffer_size=len(train_df_part)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
        "    val_ds = val_ds.map(lambda p, l: load_and_preprocess(p, l),\n",
        "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds, len(train_df_part), len(val_df_part)\n",
        "\n",
        "# ایجاد دیتاست‌ها برای سه سری توضیحی\n",
        "train_ds_t1, val_ds_t1, len_train_t1, len_val_t1 = create_datasets(train_data, 'Sagittal T1', batch_size=8)\n",
        "train_ds_t2, val_ds_t2, len_train_t2, len_val_t2 = create_datasets(train_data, 'Axial T2', batch_size=8)\n",
        "train_ds_t2stir, val_ds_t2stir, len_train_t2stir, len_val_t2stir = create_datasets(train_data, 'Sagittal T2/STIR', batch_size=8)\n",
        "\n",
        "# ------------------------------\n",
        "# 6. تعریف مدل VGG19 با استفاده از Keras (TensorFlow)\n",
        "# ------------------------------\n",
        "def create_vgg19_model(num_classes=3):\n",
        "    base_model = tf.keras.applications.VGG19(include_top=False,\n",
        "                                             input_shape=(224, 224, 3),\n",
        "                                             weights='imagenet')\n",
        "    base_model.trainable = False\n",
        "    x = layers.Flatten()(base_model.output)\n",
        "    x = layers.Dense(4096, activation='relu')(x)\n",
        "    x = layers.Dense(4096, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs=base_model.input, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# ایجاد سه مدل مجزا برای سری‌های مختلف\n",
        "model_t1 = create_vgg19_model(num_classes=3)\n",
        "model_t2 = create_vgg19_model(num_classes=3)\n",
        "model_t2stir = create_vgg19_model(num_classes=3)\n",
        "\n",
        "model_t1.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "                 loss='sparse_categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "model_t2.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "                 loss='sparse_categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "model_t2stir.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "                     loss='sparse_categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "# ------------------------------\n",
        "# 7. آموزش مدل‌ها\n",
        "# ------------------------------\n",
        "# تغییر پسوند فایل‌های checkpoint به .keras برای سازگاری با فرمت Keras\n",
        "es_callback_t1 = callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "ckpt_callback_t1 = callbacks.ModelCheckpoint('best_model_t1.keras', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "es_callback_t2 = callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "ckpt_callback_t2 = callbacks.ModelCheckpoint('best_model_t2.keras', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "es_callback_t2stir = callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "ckpt_callback_t2stir = callbacks.ModelCheckpoint('best_model_t2stir.keras', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "print(\"آموزش مدل Sagittal T1\")\n",
        "history_t1 = model_t1.fit(train_ds_t1, epochs=10, validation_data=val_ds_t1,\n",
        "                          callbacks=[es_callback_t1, ckpt_callback_t1])\n",
        "\n",
        "print(\"آموزش مدل Axial T2\")\n",
        "history_t2 = model_t2.fit(train_ds_t2, epochs=10, validation_data=val_ds_t2,\n",
        "                          callbacks=[es_callback_t2, ckpt_callback_t2])\n",
        "\n",
        "print(\"آموزش مدل Sagittal T2/STIR\")\n",
        "history_t2stir = model_t2stir.fit(train_ds_t2stir, epochs=10, validation_data=val_ds_t2stir,\n",
        "                                  callbacks=[es_callback_t2stir, ckpt_callback_t2stir])\n",
        "\n",
        "# ------------------------------\n",
        "# 8. پیش‌بینی روی داده‌های Test و ایجاد سابمیشن\n",
        "# ------------------------------\n",
        "condition_mapping = {\n",
        "    'Sagittal T1': {'left': 'left_neural_foraminal_narrowing', 'right': 'right_neural_foraminal_narrowing'},\n",
        "    'Axial T2': {'left': 'left_subarticular_stenosis', 'right': 'right_subarticular_stenosis'},\n",
        "    'Sagittal T2/STIR': 'spinal_canal_stenosis'\n",
        "}\n",
        "\n",
        "expanded_rows = []\n",
        "for index, row in test_desc_df.iterrows():\n",
        "    study_id = row['study_id']\n",
        "    series_id = row['series_id']\n",
        "    series_description = row['series_description']\n",
        "    series_path = os.path.join(train_path, 'test_images', str(study_id), str(series_id))\n",
        "    if os.path.exists(series_path):\n",
        "        image_files = [os.path.join(series_path, f) for f in os.listdir(series_path)\n",
        "                       if os.path.isfile(os.path.join(series_path, f))]\n",
        "        conditions = condition_mapping.get(series_description, {})\n",
        "        if isinstance(conditions, str):\n",
        "            conditions = {'left': conditions, 'right': conditions}\n",
        "        for side, condition in conditions.items():\n",
        "            for image_path in image_files:\n",
        "                expanded_rows.append({\n",
        "                    'study_id': study_id,\n",
        "                    'series_id': series_id,\n",
        "                    'series_description': series_description,\n",
        "                    'image_path': image_path,\n",
        "                    'condition': condition,\n",
        "                    'row_id': f\"{study_id}_{condition}\"\n",
        "                })\n",
        "\n",
        "expanded_test_desc = pd.DataFrame(expanded_rows)\n",
        "\n",
        "# به‌روزرسانی row_id با اضافه کردن سطح (level)\n",
        "levels = ['l1_l2', 'l2_l3', 'l3_l4', 'l4_l5', 'l5_s1']\n",
        "def update_row_id(row, levels):\n",
        "    level = levels[row.name % len(levels)]\n",
        "    return f\"{row['study_id']}_{row['condition']}_{level}\"\n",
        "\n",
        "expanded_test_desc['row_id'] = expanded_test_desc.apply(lambda row: update_row_id(row, levels), axis=1)\n",
        "\n",
        "# ایجاد دیتاست test (بدون برچسب)\n",
        "test_paths = expanded_test_desc['image_path'].values\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(test_paths)\n",
        "test_ds = test_ds.map(lambda p: load_and_preprocess(p),\n",
        "                      num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(1)\n",
        "\n",
        "# دیکشنری مدل‌ها بر اساس سری توضیحی\n",
        "models_dict = {\n",
        "    'Sagittal T1': model_t1,\n",
        "    'Axial T2': model_t2,\n",
        "    'Sagittal T2/STIR': model_t2stir\n",
        "}\n",
        "\n",
        "normal_mild_probs = []\n",
        "moderate_probs = []\n",
        "severe_probs = []\n",
        "predictions_list = []\n",
        "\n",
        "for i, batch in enumerate(tqdm(test_ds)):\n",
        "    series_description = expanded_test_desc.iloc[i]['series_description']\n",
        "    model_used = models_dict.get(series_description, None)\n",
        "    if model_used is None:\n",
        "        normal_mild_probs.append(None)\n",
        "        moderate_probs.append(None)\n",
        "        severe_probs.append(None)\n",
        "        predictions_list.append(None)\n",
        "    else:\n",
        "        preds = model_used.predict(batch)\n",
        "        preds = preds[0]\n",
        "        normal_mild_probs.append(preds[0])\n",
        "        moderate_probs.append(preds[1])\n",
        "        severe_probs.append(preds[2])\n",
        "        predictions_list.append(preds)\n",
        "\n",
        "expanded_test_desc['normal_mild'] = normal_mild_probs\n",
        "expanded_test_desc['moderate'] = moderate_probs\n",
        "expanded_test_desc['severe'] = severe_probs\n",
        "\n",
        "submission_df = expanded_test_desc[[\"row_id\", \"normal_mild\", \"moderate\", \"severe\"]]\n",
        "grouped_submission = submission_df.groupby('row_id').max().reset_index()\n",
        "\n",
        "sub[['normal_mild', 'moderate', 'severe']] = grouped_submission[['normal_mild', 'moderate', 'severe']]\n",
        "sub.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
        "\n",
        "print(\"نمونه سابمیشن:\")\n",
        "print(sub.head())\n",
        "\n",
        "# ------------------------------\n",
        "# 9. ذخیره مدل‌ها\n",
        "# ------------------------------\n",
        "model_t1.save(\"Vgg19_t1.keras\")\n",
        "model_t2.save(\"Vgg19_t2.keras\")\n",
        "model_t2stir.save(\"Vgg19_t2stir.keras\")\n"
      ],
      "metadata": {
        "id": "-tseZ4MG7bF9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}